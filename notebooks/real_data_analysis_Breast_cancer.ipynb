{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyObdTDQYm3vQZrsaqV2rM2E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yu-hidaka/AD-DIFFI/blob/main/notebooks/real_data_analysis_Breast_cancer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5FlbNlFNpSf9",
        "outputId": "4bdec3e2-b30d-445b-f24a-a2dcbc9b68ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m349.3/349.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.3/117.3 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for autograd-gamma (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "All dependencies installed.\n",
            "======================================================================\n",
            "AD-DIFFI Real Data Analysis: Breast Cancer (Table S2)\n",
            "======================================================================\n",
            "Downloading Breast Cancer dataset from Kaggle...\n",
            "\n",
            "[SETUP INSTRUCTIONS]\n",
            "1. Visit: https://www.kaggle.com/settings/account\n",
            "2. Click 'Create New API Token'\n",
            "3. Download kaggle.json\n",
            "4. Upload the file when prompted below\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8abccba0-dd69-411f-9527-0a073d125c84\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-8abccba0-dd69-411f-9527-0a073d125c84\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "\n",
            "Downloading...\n",
            "Dataset URL: https://www.kaggle.com/datasets/reihanenamdari/breast-cancer\n",
            "License(s): Attribution 4.0 International (CC BY 4.0)\n",
            "Downloading breast-cancer.zip to /tmp/ad_diffi_data\n",
            "  0% 0.00/42.8k [00:00<?, ?B/s]\n",
            "100% 42.8k/42.8k [00:00<00:00, 118MB/s]\n",
            "[SUCCESS] Dataset saved to: /tmp/ad_diffi_data/Breast_Cancer.csv\n",
            "[INFO] Dataset: 4024 samples, 14 features\n",
            "[INFO] Feature types: 4 continuous, 10 binary\n",
            "======================================================================\n",
            "Running feature importance comparison...\n",
            "======================================================================\n",
            "Establishing noise baseline (RSO DIFFI) over 20 iterations...\n",
            "  Continuous (mean=1.0365, std=0.0069)\n",
            "  Binary      (mean=1.0672, std=0.0175)\n",
            "Running 20 iterations for feature importance comparison...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2418914820.py:236: RuntimeWarning: invalid value encountered in divide\n",
            "  fi_outliers = np.where(cnt_outliers > 0, cfi_outliers / cnt_outliers, 0)\n",
            "/tmp/ipython-input-2418914820.py:237: RuntimeWarning: invalid value encountered in divide\n",
            "  fi_inliers = np.where(cnt_inliers > 0, cfi_inliers / cnt_inliers, 0)\n",
            "/tmp/ipython-input-2418914820.py:236: RuntimeWarning: invalid value encountered in divide\n",
            "  fi_outliers = np.where(cnt_outliers > 0, cfi_outliers / cnt_outliers, 0)\n",
            "/tmp/ipython-input-2418914820.py:237: RuntimeWarning: invalid value encountered in divide\n",
            "  fi_inliers = np.where(cnt_inliers > 0, cfi_inliers / cnt_inliers, 0)\n",
            "/tmp/ipython-input-2418914820.py:236: RuntimeWarning: invalid value encountered in divide\n",
            "  fi_outliers = np.where(cnt_outliers > 0, cfi_outliers / cnt_outliers, 0)\n",
            "/tmp/ipython-input-2418914820.py:237: RuntimeWarning: invalid value encountered in divide\n",
            "  fi_inliers = np.where(cnt_inliers > 0, cfi_inliers / cnt_inliers, 0)\n",
            "/tmp/ipython-input-2418914820.py:236: RuntimeWarning: invalid value encountered in divide\n",
            "  fi_outliers = np.where(cnt_outliers > 0, cfi_outliers / cnt_outliers, 0)\n",
            "/tmp/ipython-input-2418914820.py:237: RuntimeWarning: invalid value encountered in divide\n",
            "  fi_inliers = np.where(cnt_inliers > 0, cfi_inliers / cnt_inliers, 0)\n",
            "/tmp/ipython-input-2418914820.py:236: RuntimeWarning: invalid value encountered in divide\n",
            "  fi_outliers = np.where(cnt_outliers > 0, cfi_outliers / cnt_outliers, 0)\n",
            "/tmp/ipython-input-2418914820.py:237: RuntimeWarning: invalid value encountered in divide\n",
            "  fi_inliers = np.where(cnt_inliers > 0, cfi_inliers / cnt_inliers, 0)\n",
            "/tmp/ipython-input-2418914820.py:236: RuntimeWarning: invalid value encountered in divide\n",
            "  fi_outliers = np.where(cnt_outliers > 0, cfi_outliers / cnt_outliers, 0)\n",
            "/tmp/ipython-input-2418914820.py:237: RuntimeWarning: invalid value encountered in divide\n",
            "  fi_inliers = np.where(cnt_inliers > 0, cfi_inliers / cnt_inliers, 0)\n",
            "/tmp/ipython-input-2418914820.py:236: RuntimeWarning: invalid value encountered in divide\n",
            "  fi_outliers = np.where(cnt_outliers > 0, cfi_outliers / cnt_outliers, 0)\n",
            "/tmp/ipython-input-2418914820.py:237: RuntimeWarning: invalid value encountered in divide\n",
            "  fi_inliers = np.where(cnt_inliers > 0, cfi_inliers / cnt_inliers, 0)\n",
            "/tmp/ipython-input-2418914820.py:236: RuntimeWarning: invalid value encountered in divide\n",
            "  fi_outliers = np.where(cnt_outliers > 0, cfi_outliers / cnt_outliers, 0)\n",
            "/tmp/ipython-input-2418914820.py:237: RuntimeWarning: invalid value encountered in divide\n",
            "  fi_inliers = np.where(cnt_inliers > 0, cfi_inliers / cnt_inliers, 0)\n",
            "/tmp/ipython-input-2418914820.py:236: RuntimeWarning: invalid value encountered in divide\n",
            "  fi_outliers = np.where(cnt_outliers > 0, cfi_outliers / cnt_outliers, 0)\n",
            "/tmp/ipython-input-2418914820.py:237: RuntimeWarning: invalid value encountered in divide\n",
            "  fi_inliers = np.where(cnt_inliers > 0, cfi_inliers / cnt_inliers, 0)\n",
            "/tmp/ipython-input-2418914820.py:236: RuntimeWarning: invalid value encountered in divide\n",
            "  fi_outliers = np.where(cnt_outliers > 0, cfi_outliers / cnt_outliers, 0)\n",
            "/tmp/ipython-input-2418914820.py:237: RuntimeWarning: invalid value encountered in divide\n",
            "  fi_inliers = np.where(cnt_inliers > 0, cfi_inliers / cnt_inliers, 0)\n",
            "/tmp/ipython-input-2418914820.py:236: RuntimeWarning: invalid value encountered in divide\n",
            "  fi_outliers = np.where(cnt_outliers > 0, cfi_outliers / cnt_outliers, 0)\n",
            "/tmp/ipython-input-2418914820.py:237: RuntimeWarning: invalid value encountered in divide\n",
            "  fi_inliers = np.where(cnt_inliers > 0, cfi_inliers / cnt_inliers, 0)\n",
            "/tmp/ipython-input-2418914820.py:236: RuntimeWarning: invalid value encountered in divide\n",
            "  fi_outliers = np.where(cnt_outliers > 0, cfi_outliers / cnt_outliers, 0)\n",
            "/tmp/ipython-input-2418914820.py:237: RuntimeWarning: invalid value encountered in divide\n",
            "  fi_inliers = np.where(cnt_inliers > 0, cfi_inliers / cnt_inliers, 0)\n",
            "/tmp/ipython-input-2418914820.py:236: RuntimeWarning: invalid value encountered in divide\n",
            "  fi_outliers = np.where(cnt_outliers > 0, cfi_outliers / cnt_outliers, 0)\n",
            "/tmp/ipython-input-2418914820.py:237: RuntimeWarning: invalid value encountered in divide\n",
            "  fi_inliers = np.where(cnt_inliers > 0, cfi_inliers / cnt_inliers, 0)\n",
            "/tmp/ipython-input-2418914820.py:236: RuntimeWarning: invalid value encountered in divide\n",
            "  fi_outliers = np.where(cnt_outliers > 0, cfi_outliers / cnt_outliers, 0)\n",
            "/tmp/ipython-input-2418914820.py:237: RuntimeWarning: invalid value encountered in divide\n",
            "  fi_inliers = np.where(cnt_inliers > 0, cfi_inliers / cnt_inliers, 0)\n",
            "/tmp/ipython-input-2418914820.py:236: RuntimeWarning: invalid value encountered in divide\n",
            "  fi_outliers = np.where(cnt_outliers > 0, cfi_outliers / cnt_outliers, 0)\n",
            "/tmp/ipython-input-2418914820.py:237: RuntimeWarning: invalid value encountered in divide\n",
            "  fi_inliers = np.where(cnt_inliers > 0, cfi_inliers / cnt_inliers, 0)\n",
            "/tmp/ipython-input-2418914820.py:236: RuntimeWarning: invalid value encountered in divide\n",
            "  fi_outliers = np.where(cnt_outliers > 0, cfi_outliers / cnt_outliers, 0)\n",
            "/tmp/ipython-input-2418914820.py:237: RuntimeWarning: invalid value encountered in divide\n",
            "  fi_inliers = np.where(cnt_inliers > 0, cfi_inliers / cnt_inliers, 0)\n",
            "/tmp/ipython-input-2418914820.py:236: RuntimeWarning: invalid value encountered in divide\n",
            "  fi_outliers = np.where(cnt_outliers > 0, cfi_outliers / cnt_outliers, 0)\n",
            "/tmp/ipython-input-2418914820.py:237: RuntimeWarning: invalid value encountered in divide\n",
            "  fi_inliers = np.where(cnt_inliers > 0, cfi_inliers / cnt_inliers, 0)\n",
            "/tmp/ipython-input-2418914820.py:236: RuntimeWarning: invalid value encountered in divide\n",
            "  fi_outliers = np.where(cnt_outliers > 0, cfi_outliers / cnt_outliers, 0)\n",
            "/tmp/ipython-input-2418914820.py:237: RuntimeWarning: invalid value encountered in divide\n",
            "  fi_inliers = np.where(cnt_inliers > 0, cfi_inliers / cnt_inliers, 0)\n",
            "/tmp/ipython-input-2418914820.py:236: RuntimeWarning: invalid value encountered in divide\n",
            "  fi_outliers = np.where(cnt_outliers > 0, cfi_outliers / cnt_outliers, 0)\n",
            "/tmp/ipython-input-2418914820.py:237: RuntimeWarning: invalid value encountered in divide\n",
            "  fi_inliers = np.where(cnt_inliers > 0, cfi_inliers / cnt_inliers, 0)\n",
            "/tmp/ipython-input-2418914820.py:236: RuntimeWarning: invalid value encountered in divide\n",
            "  fi_outliers = np.where(cnt_outliers > 0, cfi_outliers / cnt_outliers, 0)\n",
            "/tmp/ipython-input-2418914820.py:237: RuntimeWarning: invalid value encountered in divide\n",
            "  fi_inliers = np.where(cnt_inliers > 0, cfi_inliers / cnt_inliers, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Feature Importance Comparison (Table S2)\n",
            "| Feature                | Type   |   Original_DIFFI |   AD_DIFFI_RSO_Z |   Rank_Original |   Rank_AD_DIFFI |   Rank_Change |\n",
            "|:-----------------------|:-------|-----------------:|-----------------:|----------------:|----------------:|--------------:|\n",
            "| A_Stage_Regional       | bin    |           1.0871 |          18.4214 |               2 |               1 |            -1 |\n",
            "| Estrogen_Positive      | bin    |           0.8188 |          16.7249 |              11 |               2 |            -9 |\n",
            "| Race_White             | bin    |           0.8516 |          13.3375 |               8 |               3 |            -5 |\n",
            "| Progesterone_Positive  | bin    |           0.8705 |          12.6956 |               7 |               4 |            -3 |\n",
            "| Marital_Married        | bin    |           1.0404 |          12.0906 |               3 |               5 |             2 |\n",
            "| Differentiate_High     | bin    |           0.9939 |          12.0115 |               4 |               6 |             2 |\n",
            "| Reginol Node Positive  | cont   |           0.9451 |          11.915  |               5 |               7 |             2 |\n",
            "| T_Stage_T1             | bin    |           1.2111 |          11.5759 |               1 |               8 |             7 |\n",
            "| Age                    | cont   |           0.8917 |          11.4907 |               6 |               9 |             3 |\n",
            "| Regional Node Examined | cont   |           0.8374 |          10.7185 |              10 |              10 |             0 |\n",
            "| Tumor Size             | cont   |           0.8512 |           9.8324 |               9 |              11 |             2 |\n",
            "| N_Stage_N0             | bin    |           0      |         -61.0502 |              13 |              13 |             0 |\n",
            "| Stage_II               | bin    |           0      |         -61.0502 |              13 |              13 |             0 |\n",
            "| Grade_Low              | bin    |           0      |         -61.0502 |              13 |              13 |             0 |\n",
            "======================================================================\n",
            "Isolation Forest AUC Evaluation\n",
            "======================================================================\n",
            "IF AUC (all 14 features): 0.6681\n",
            "IF AUC (Original DIFFI top-6): 0.6628\n",
            "IF AUC (AD-DIFFI top-6): 0.6360\n",
            "======================================================================\n",
            "Analysis complete.\n",
            "======================================================================\n",
            "[INFO] Results saved to: breast_cancer_analysis_results.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a76bdc83-1972-4fa6-ae38-37d31211278b\", \"breast_cancer_analysis_results.csv\", 731)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Download started...\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# Real Data Analysis: Breast Cancer (Table S2)\n",
        "# Comparison of Original DIFFI vs AD-DIFFI (RSO + Z-normalization)\n",
        "# For Google Colab Execution\n",
        "# ============================================================================\n",
        "\n",
        "# =============================================================================\n",
        "# Cell 1: Setup and Install Dependencies\n",
        "# =============================================================================\n",
        "\n",
        "!pip install -q kaggle scikit-learn pandas numpy matplotlib seaborn scipy lifelines\n",
        "print(\"All dependencies installed.\")\n",
        "\n",
        "# =============================================================================\n",
        "# Cell 2: Data Setup Function for Colab\n",
        "# =============================================================================\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "\n",
        "def setup_colab_data():\n",
        "    \"\"\"\n",
        "    Automatic data download for Colab environment.\n",
        "\n",
        "    Supports two methods:\n",
        "    1. Kaggle API: Download from Kaggle directly\n",
        "    2. Google Drive: Manual upload to Drive\n",
        "\n",
        "    Returns:\n",
        "        str: Path to Breast_Cancer.csv file\n",
        "    \"\"\"\n",
        "    data_dir = Path('/tmp/ad_diffi_data')\n",
        "    data_dir.mkdir(exist_ok=True)\n",
        "    csv_file = data_dir / 'Breast_Cancer.csv'\n",
        "\n",
        "    # Check if dataset already exists\n",
        "    if csv_file.exists():\n",
        "        print(\"[INFO] Dataset already downloaded at {}\".format(csv_file))\n",
        "        return str(csv_file)\n",
        "\n",
        "    print(\"Downloading Breast Cancer dataset from Kaggle...\")\n",
        "    print(\"\\n[SETUP INSTRUCTIONS]\")\n",
        "    print(\"1. Visit: https://www.kaggle.com/settings/account\")\n",
        "    print(\"2. Click 'Create New API Token'\")\n",
        "    print(\"3. Download kaggle.json\")\n",
        "    print(\"4. Upload the file when prompted below\\n\")\n",
        "\n",
        "    # File upload dialog\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    if 'kaggle.json' not in uploaded:\n",
        "        print(\"[ERROR] kaggle.json not found\")\n",
        "        return None\n",
        "\n",
        "    # Configure Kaggle\n",
        "    import subprocess\n",
        "    subprocess.run(['pip', 'install', '-q', 'kaggle'], check=True)\n",
        "    subprocess.run(['mkdir', '-p', str(Path.home() / '.kaggle')], check=True)\n",
        "    subprocess.run(['mv', 'kaggle.json', str(Path.home() / '.kaggle' / 'kaggle.json')], check=True)\n",
        "    subprocess.run(['chmod', '600', str(Path.home() / '.kaggle' / 'kaggle.json')], check=True)\n",
        "\n",
        "    # Download dataset (修正後)\n",
        "    print(\"\\nDownloading...\")\n",
        "    !kaggle datasets download -d reihanenamdari/breast-cancer -p {str(data_dir)} --unzip --force\n",
        "\n",
        "    if csv_file.exists():\n",
        "        print(\"[SUCCESS] Dataset saved to: {}\".format(csv_file))\n",
        "        return str(csv_file)\n",
        "    else:\n",
        "        print(\"[ERROR] Download failed. Check dataset slug or internet.\")\n",
        "        print(\"Alternative: Manual download from https://www.kaggle.com/datasets/reihanenamdari/breast-cancer\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Cell 3: Core Functions - Original DIFFI Implementation\n",
        "# =============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "from math import ceil\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from typing import Tuple\n",
        "\n",
        "def _get_iic(estimator, predictions, is_leaves, adjust_iic: bool = True) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Compute Induced Imbalance Coefficient (IIC).\n",
        "\n",
        "    Args:\n",
        "        estimator: Fitted isolation tree estimator\n",
        "        predictions: Input samples\n",
        "        is_leaves: Boolean array marking leaf nodes\n",
        "        adjust_iic: Whether to normalize IIC to [0.5, 1.0]\n",
        "\n",
        "    Returns:\n",
        "        lambda_: IIC values for each node\n",
        "    \"\"\"\n",
        "    desired_min = 0.5\n",
        "    desired_max = 1.0\n",
        "    epsilon = 0.0\n",
        "\n",
        "    n_nodes = estimator.tree_.node_count\n",
        "    lambda_ = np.zeros(n_nodes)\n",
        "    children_left = estimator.tree_.children_left\n",
        "    children_right = estimator.tree_.children_right\n",
        "\n",
        "    if predictions.shape[0] == 0:\n",
        "        return lambda_\n",
        "\n",
        "    # Compute sample counts in each node\n",
        "    node_indicator = estimator.decision_path(predictions).toarray()\n",
        "    num_samples_in_node = np.sum(node_indicator, axis=0)\n",
        "\n",
        "    for node in range(n_nodes):\n",
        "        num_samples_current = num_samples_in_node[node]\n",
        "        num_samples_left = num_samples_in_node[children_left[node]]\n",
        "        num_samples_right = num_samples_in_node[children_right[node]]\n",
        "\n",
        "        if num_samples_current <= 1 or is_leaves[node]:\n",
        "            lambda_[node] = -1\n",
        "        elif num_samples_left == 0 or num_samples_right == 0:\n",
        "            lambda_[node] = epsilon\n",
        "        else:\n",
        "            # Compute imbalance ratio\n",
        "            if num_samples_current % 2 == 0:\n",
        "                current_min = 0.5\n",
        "            else:\n",
        "                current_min = ceil(num_samples_current / 2) / num_samples_current\n",
        "            current_max = (num_samples_current - 1) / num_samples_current\n",
        "            max_child_ratio = max(num_samples_left, num_samples_right) / num_samples_current\n",
        "\n",
        "            if adjust_iic and current_min != current_max:\n",
        "                lambda_[node] = ((max_child_ratio - current_min) / (current_max - current_min)) \\\n",
        "                               * (desired_max - desired_min) + desired_min\n",
        "            else:\n",
        "                lambda_[node] = max_child_ratio\n",
        "\n",
        "    return lambda_\n",
        "\n",
        "\n",
        "def diffi_ib(iforest: IsolationForest, X: np.ndarray, adjust_iic: bool = True) -> Tuple[np.ndarray, float]:\n",
        "    \"\"\"\n",
        "    Original DIFFI: Depth-based Isolation Forest Feature Importance.\n",
        "\n",
        "    Args:\n",
        "        iforest: Fitted IsolationForest model\n",
        "        X: Input data (n_samples, n_features)\n",
        "        adjust_iic: Whether to adjust IIC scaling\n",
        "\n",
        "    Returns:\n",
        "        fi_ib: Feature importance scores (n_features,)\n",
        "        exec_time: Computation time in seconds\n",
        "    \"\"\"\n",
        "    start = time.time()\n",
        "    num_features = X.shape[1]\n",
        "    estimators = iforest.estimators_\n",
        "    in_bag_samples = iforest.estimators_samples_\n",
        "\n",
        "    # Initialize accumulators\n",
        "    cfi_outliers = np.zeros(num_features, dtype=float)\n",
        "    cfi_inliers = np.zeros(num_features, dtype=float)\n",
        "    cnt_outliers = np.zeros(num_features, dtype=int)\n",
        "    cnt_inliers = np.zeros(num_features, dtype=int)\n",
        "\n",
        "    # Compute anomaly threshold\n",
        "    global_scores = iforest.decision_function(X)\n",
        "    threshold = np.percentile(global_scores, 100 * iforest.contamination)\n",
        "\n",
        "    for k, estimator in enumerate(estimators):\n",
        "        in_bag_index = list(in_bag_samples[k])\n",
        "        X_ib = X[in_bag_index, :]\n",
        "        scores_ib = global_scores[in_bag_index]\n",
        "\n",
        "        X_outliers = X_ib[scores_ib < threshold]\n",
        "        X_inliers = X_ib[scores_ib >= threshold]\n",
        "\n",
        "        if X_inliers.shape[0] == 0 or X_outliers.shape[0] == 0:\n",
        "            continue\n",
        "\n",
        "        # Extract tree structure\n",
        "        tree = estimator.tree_\n",
        "        n_nodes = tree.node_count\n",
        "        children_left = tree.children_left\n",
        "        children_right = tree.children_right\n",
        "        feature = tree.feature\n",
        "\n",
        "        # Compute node depths\n",
        "        node_depth = np.zeros(n_nodes, dtype=np.int64)\n",
        "        is_leaves = np.zeros(n_nodes, dtype=bool)\n",
        "\n",
        "        stack = [(0, 0)]\n",
        "        while stack:\n",
        "            node_id, depth = stack.pop()\n",
        "            node_depth[node_id] = depth\n",
        "            if children_left[node_id] != children_right[node_id]:\n",
        "                stack.append((children_left[node_id], depth + 1))\n",
        "                stack.append((children_right[node_id], depth + 1))\n",
        "            else:\n",
        "                is_leaves[node_id] = True\n",
        "\n",
        "        # Process outliers\n",
        "        lambda_outliers = _get_iic(estimator, X_outliers, is_leaves, adjust_iic)\n",
        "        node_indicator_outliers = estimator.decision_path(X_outliers).toarray()\n",
        "\n",
        "        for i in range(len(X_outliers)):\n",
        "            path = np.where(node_indicator_outliers[i] == 1)[0]\n",
        "            if len(path) == 0:\n",
        "                continue\n",
        "            depth = node_depth[path[-1]]\n",
        "            for node in path:\n",
        "                current_feature = feature[node]\n",
        "                if current_feature >= 0 and lambda_outliers[node] != -1:\n",
        "                    cfi_outliers[current_feature] += (1.0 / depth) * lambda_outliers[node]\n",
        "                    cnt_outliers[current_feature] += 1\n",
        "\n",
        "        # Process inliers\n",
        "        lambda_inliers = _get_iic(estimator, X_inliers, is_leaves, adjust_iic)\n",
        "        node_indicator_inliers = estimator.decision_path(X_inliers).toarray()\n",
        "\n",
        "        for i in range(len(X_inliers)):\n",
        "            path = np.where(node_indicator_inliers[i] == 1)[0]\n",
        "            if len(path) == 0:\n",
        "                continue\n",
        "            depth = node_depth[path[-1]]\n",
        "            for node in path:\n",
        "                current_feature = feature[node]\n",
        "                if current_feature >= 0 and lambda_inliers[node] != -1:\n",
        "                    cfi_inliers[current_feature] += (1.0 / depth) * lambda_inliers[node]\n",
        "                    cnt_inliers[current_feature] += 1\n",
        "\n",
        "    # Normalize and compute ratio\n",
        "    fi_outliers = np.where(cnt_outliers > 0, cfi_outliers / cnt_outliers, 0)\n",
        "    fi_inliers = np.where(cnt_inliers > 0, cfi_inliers / cnt_inliers, 0)\n",
        "\n",
        "    with np.errstate(divide='ignore', invalid='ignore'):\n",
        "        fi_ib = np.divide(fi_outliers, fi_inliers, out=np.zeros_like(fi_outliers),\n",
        "                         where=fi_inliers != 0)\n",
        "\n",
        "    exec_time = time.time() - start\n",
        "    return fi_ib, exec_time\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Cell 4: AD-DIFFI Implementation (RSO + Z-normalization)\n",
        "# =============================================================================\n",
        "\n",
        "from typing import Dict, Tuple\n",
        "\n",
        "MIN_DEPTH = 1\n",
        "\n",
        "def diffi_ib_binary_rso(\n",
        "    iforest: IsolationForest,\n",
        "    X_data: np.ndarray,\n",
        "    feature_types: Dict[int, str]\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    AD-DIFFI with Root-Split-Only (RSO) constraint for binary features.\n",
        "\n",
        "    Args:\n",
        "        iforest: Fitted IsolationForest model\n",
        "        X_data: Input data (n_samples, n_features)\n",
        "        feature_types: Dict mapping feature index to 'cont' or 'bin'\n",
        "\n",
        "    Returns:\n",
        "        fi_rso: Feature importance scores with RSO constraint\n",
        "    \"\"\"\n",
        "    num_features = X_data.shape[1]\n",
        "    estimators = iforest.estimators_\n",
        "\n",
        "    cfi_outliers = np.zeros(num_features, dtype=float)\n",
        "    cfi_inliers = np.zeros(num_features, dtype=float)\n",
        "    cnt_outliers = np.zeros(num_features, dtype=int)\n",
        "    cnt_inliers = np.zeros(num_features, dtype=int)\n",
        "\n",
        "    # Compute anomaly threshold\n",
        "    global_scores = iforest.decision_function(X_data)\n",
        "    threshold = np.percentile(global_scores, 100 * iforest.contamination)\n",
        "\n",
        "    for k, estimator in enumerate(estimators):\n",
        "        in_bag_index = iforest.estimators_samples_[k]\n",
        "        X_ib = X_data[in_bag_index]\n",
        "        scores_ib = global_scores[in_bag_index]\n",
        "\n",
        "        X_outliers = X_ib[scores_ib < threshold]\n",
        "        X_inliers = X_ib[scores_ib >= threshold]\n",
        "\n",
        "        if X_outliers.shape[0] == 0 or X_inliers.shape[0] == 0:\n",
        "            continue\n",
        "\n",
        "        # Extract tree structure\n",
        "        tree = estimator.tree_\n",
        "        n_nodes = tree.node_count\n",
        "        children_left = tree.children_left\n",
        "        children_right = tree.children_right\n",
        "        feature_index = tree.feature\n",
        "        n_node_samples = tree.n_node_samples\n",
        "\n",
        "        # Compute node depths\n",
        "        node_depth = np.zeros(n_nodes, dtype=int)\n",
        "        stack = [(0, 0)]\n",
        "        while stack:\n",
        "            node_id, depth = stack.pop()\n",
        "            node_depth[node_id] = depth\n",
        "            if children_left[node_id] != children_right[node_id]:\n",
        "                stack.append((children_left[node_id], depth + 1))\n",
        "                stack.append((children_right[node_id], depth + 1))\n",
        "\n",
        "        # Compute adjusted lambda (imbalance coefficient)\n",
        "        lambda_adjusted = np.zeros(n_nodes, dtype=float)\n",
        "        for node in range(n_nodes):\n",
        "            if children_left[node] != children_right[node]:\n",
        "                n_parent = n_node_samples[node]\n",
        "                n_left = n_node_samples[children_left[node]]\n",
        "                n_right = n_node_samples[children_right[node]]\n",
        "                ratio_small = min(n_left, n_right) / n_parent\n",
        "                lambda_adjusted[node] = 1.0 - ratio_small\n",
        "\n",
        "        def accumulate_contributions(X_subset, cfi, cnt):\n",
        "            \"\"\"Accumulate feature importance contributions from samples.\"\"\"\n",
        "            node_indicator = estimator.decision_path(X_subset).toarray()\n",
        "            for sample_idx in range(X_subset.shape[0]):\n",
        "                path = np.where(node_indicator[sample_idx] == 1)[0]\n",
        "                if len(path) == 0:\n",
        "                    continue\n",
        "\n",
        "                depth_leaf = node_depth[path[-1]]\n",
        "                h_leaf = max(depth_leaf, MIN_DEPTH)\n",
        "                depth_scale = 1.0 / h_leaf\n",
        "\n",
        "                for node in path:\n",
        "                    feat_idx = feature_index[node]\n",
        "                    if feat_idx < 0:\n",
        "                        continue\n",
        "\n",
        "                    feature_type = feature_types[feat_idx]\n",
        "                    # RSO constraint: binary features only split at root (depth=0)\n",
        "                    if feature_type == 'bin' and node_depth[node] > 0:\n",
        "                        continue\n",
        "\n",
        "                    contribution = depth_scale * lambda_adjusted[node]\n",
        "                    cfi[feat_idx] += contribution\n",
        "                    cnt[feat_idx] += 1\n",
        "\n",
        "        accumulate_contributions(X_outliers, cfi_outliers, cnt_outliers)\n",
        "        accumulate_contributions(X_inliers, cfi_inliers, cnt_inliers)\n",
        "\n",
        "    # Normalize and compute ratio\n",
        "    with np.errstate(divide='ignore', invalid='ignore'):\n",
        "        fi_outliers = np.where(cnt_outliers > 0, cfi_outliers / cnt_outliers, 0)\n",
        "        fi_inliers = np.where(cnt_inliers > 0, cfi_inliers / cnt_inliers, 0)\n",
        "        fi_rso = np.divide(fi_outliers, fi_inliers, out=np.zeros_like(fi_outliers),\n",
        "                          where=fi_inliers != 0)\n",
        "\n",
        "    return fi_rso\n",
        "\n",
        "\n",
        "def noise_baseline_rso(\n",
        "    X_dim: int,\n",
        "    feature_types: Dict[int, str],\n",
        "    if_params: Dict,\n",
        "    n_iter: int = 50\n",
        ") -> Tuple[float, float, float, float]:\n",
        "    \"\"\"\n",
        "    Establish noise baseline for RSO-based DIFFI scores.\n",
        "\n",
        "    Uses random uniform noise to determine null distribution.\n",
        "\n",
        "    Args:\n",
        "        X_dim: Number of features\n",
        "        feature_types: Dict mapping feature index to 'cont' or 'bin'\n",
        "        if_params: Isolation Forest hyperparameters\n",
        "        n_iter: Number of iterations\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (cont_mean, cont_sd, bin_mean, bin_sd)\n",
        "    \"\"\"\n",
        "    print(\"Establishing noise baseline (RSO DIFFI) over {} iterations...\".format(n_iter))\n",
        "    scores_list = []\n",
        "\n",
        "    cont_indices = [i for i, t in feature_types.items() if t == 'cont']\n",
        "    bin_indices = [i for i, t in feature_types.items() if t == 'bin']\n",
        "\n",
        "    for k in range(n_iter):\n",
        "        X_noise = np.random.uniform(0, 1, size=(2000, X_dim))\n",
        "        iforest = IsolationForest(random_state=k, **if_params)\n",
        "        iforest.fit(X_noise)\n",
        "        fi = diffi_ib_binary_rso(iforest, X_noise, feature_types)\n",
        "        scores_list.append(fi)\n",
        "\n",
        "    M = np.vstack(scores_list)\n",
        "\n",
        "    cont_scores = M[:, cont_indices] if cont_indices else np.array([])\n",
        "    bin_scores = M[:, bin_indices] if bin_indices else np.array([])\n",
        "\n",
        "    cont_mean = float(cont_scores.mean()) if len(cont_indices) > 0 else 1.0\n",
        "    cont_sd = float(cont_scores.std()) if len(cont_indices) > 0 else 1.0\n",
        "    bin_mean = float(bin_scores.mean()) if len(bin_indices) > 0 else 1.0\n",
        "    bin_sd = float(bin_scores.std()) if len(bin_indices) > 0 else 1.0\n",
        "\n",
        "    print(\"  Continuous (mean={:.4f}, std={:.4f})\".format(cont_mean, cont_sd))\n",
        "    print(\"  Binary      (mean={:.4f}, std={:.4f})\".format(bin_mean, bin_sd))\n",
        "\n",
        "    return cont_mean, cont_sd, bin_mean, bin_sd\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Cell 5: Comparison Function\n",
        "# =============================================================================\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from typing import List\n",
        "\n",
        "def compare_diffi_vs_ad_diffi(\n",
        "    X: np.ndarray,\n",
        "    feature_names: List[str],\n",
        "    feature_types: Dict[int, str],\n",
        "    if_params: Dict,\n",
        "    n_iter_noise: int = 50,\n",
        "    n_iter_main: int = 50,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Compare Original DIFFI vs AD-DIFFI (RSO + Z-normalization).\n",
        "\n",
        "    Args:\n",
        "        X: Feature matrix (n_samples, n_features)\n",
        "        feature_names: List of feature names\n",
        "        feature_types: Dict mapping feature index to 'cont' or 'bin'\n",
        "        if_params: Isolation Forest parameters\n",
        "        n_iter_noise: Iterations for noise baseline\n",
        "        n_iter_main: Iterations for main comparison\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with importance scores, ranks and rank changes\n",
        "    \"\"\"\n",
        "    # Standardize data\n",
        "    X_scaled = StandardScaler().fit_transform(X)\n",
        "\n",
        "    # Establish noise baseline for AD-DIFFI\n",
        "    cont_mean, cont_sd, bin_mean, bin_sd = noise_baseline_rso(\n",
        "        X_dim=X_scaled.shape[1],\n",
        "        feature_types=feature_types,\n",
        "        if_params=if_params,\n",
        "        n_iter=n_iter_noise\n",
        "    )\n",
        "\n",
        "    # Run main comparison iterations\n",
        "    orig_scores_list = []\n",
        "    rso_raw_list = []\n",
        "\n",
        "    print(\"Running {} iterations for feature importance comparison...\".format(n_iter_main))\n",
        "    for k in range(n_iter_main):\n",
        "        iforest = IsolationForest(random_state=k, **if_params)\n",
        "        iforest.fit(X_scaled)\n",
        "\n",
        "        # Original DIFFI\n",
        "        fi_orig, _ = diffi_ib(iforest, X_scaled, adjust_iic=True)\n",
        "        orig_scores_list.append(fi_orig)\n",
        "\n",
        "        # AD-DIFFI RSO (raw)\n",
        "        fi_rso = diffi_ib_binary_rso(iforest, X_scaled, feature_types)\n",
        "        rso_raw_list.append(fi_rso)\n",
        "\n",
        "    mean_orig = np.mean(orig_scores_list, axis=0)\n",
        "    M_rso_raw = np.vstack(rso_raw_list)\n",
        "\n",
        "    # Z-normalize RSO scores\n",
        "    rso_z_scores = M_rso_raw.copy()\n",
        "    for i in range(len(feature_names)):\n",
        "        if feature_types[i] == 'cont':\n",
        "            if cont_sd > 0:\n",
        "                rso_z_scores[:, i] = (M_rso_raw[:, i] - cont_mean) / cont_sd\n",
        "            else:\n",
        "                rso_z_scores[:, i] = 0.0\n",
        "        else:  # binary\n",
        "            if bin_sd > 0:\n",
        "                rso_z_scores[:, i] = (M_rso_raw[:, i] - bin_mean) / bin_sd\n",
        "            else:\n",
        "                rso_z_scores[:, i] = 0.0\n",
        "\n",
        "    mean_rso_z = np.mean(rso_z_scores, axis=0)\n",
        "\n",
        "    # Build comparison table\n",
        "    compare_df = pd.DataFrame({\n",
        "        \"Feature\": feature_names,\n",
        "        \"Type\": [feature_types[i] for i in range(len(feature_names))],\n",
        "        \"Original_DIFFI\": np.round(mean_orig, 4),\n",
        "        \"AD_DIFFI_RSO_Z\": np.round(mean_rso_z, 4),\n",
        "    })\n",
        "\n",
        "    compare_df[\"Rank_Original\"] = compare_df[\"Original_DIFFI\"].rank(ascending=False)\n",
        "    compare_df[\"Rank_AD_DIFFI\"] = compare_df[\"AD_DIFFI_RSO_Z\"].rank(ascending=False)\n",
        "    compare_df[\"Rank_Change\"] = compare_df[\"Rank_AD_DIFFI\"] - compare_df[\"Rank_Original\"]\n",
        "\n",
        "    return compare_df.sort_values(\"AD_DIFFI_RSO_Z\", ascending=False)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Cell 6: Breast Cancer Dataset Processing\n",
        "# =============================================================================\n",
        "\n",
        "def create_binary_features_breast_cancer(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Convert Breast Cancer dataset to mixed continuous/binary features.\n",
        "\n",
        "    Implements specifications from Table S2.\n",
        "\n",
        "    Args:\n",
        "        df: Raw breast cancer DataFrame from Kaggle\n",
        "\n",
        "    Returns:\n",
        "        Processed DataFrame with continuous and binary features\n",
        "    \"\"\"\n",
        "    df_processed = df[[\n",
        "        'Age', 'Tumor Size', 'Regional Node Examined',\n",
        "        'Reginol Node Positive', 'Survival Months', 'Status'\n",
        "    ]].copy()\n",
        "\n",
        "    # Binary encoding features\n",
        "    df_processed['Race_White'] = (df['Race'] == 'White').astype(int)\n",
        "    df_processed['Marital_Married'] = df['Marital Status'].isin(\n",
        "        ['Married', 'Married-spouse-absent']\n",
        "    ).astype(int)\n",
        "    df_processed['T_Stage_T1'] = (df['T Stage '] == 'T1').astype(int)\n",
        "    df_processed['N_Stage_N0'] = (df['N Stage'] == 'N0').astype(int)\n",
        "    df_processed['Stage_II'] = (df['6th Stage'] == 'II').astype(int)\n",
        "\n",
        "    # Differentiation mapping\n",
        "    diff_map = {\n",
        "        'Poorly differentiated': 0,\n",
        "        'Moderately differentiated': 1,\n",
        "        'Well differentiated': 1,\n",
        "        'Poorly differentiated; Moderately differentiated': 0\n",
        "    }\n",
        "    df_processed['Differentiate_High'] = df['differentiate'].map(diff_map).fillna(1).astype(int)\n",
        "    df_processed['Grade_Low'] = df['Grade'].isin(['Grade I', 'Grade II']).astype(int)\n",
        "    df_processed['A_Stage_Regional'] = (df['A Stage'] == 'Regional').astype(int)\n",
        "    df_processed['Estrogen_Positive'] = (df['Estrogen Status'] == 'Positive').astype(int)\n",
        "    df_processed['Progesterone_Positive'] = (df['Progesterone Status'] == 'Positive').astype(int)\n",
        "\n",
        "    return df_processed\n",
        "\n",
        "\n",
        "def evaluate_if_auc(\n",
        "    df: pd.DataFrame,\n",
        "    feature_list: List[str],\n",
        "    y: np.ndarray,\n",
        "    if_params: Dict,\n",
        "    random_state: int = 0\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Evaluate Isolation Forest AUC using specified features.\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame with features\n",
        "        feature_list: List of feature columns to use\n",
        "        y: Binary target variable\n",
        "        if_params: Isolation Forest parameters\n",
        "        random_state: Random seed\n",
        "\n",
        "    Returns:\n",
        "        AUC score on test set\n",
        "    \"\"\"\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn.metrics import roc_auc_score\n",
        "\n",
        "    X = df[feature_list].fillna(df[feature_list].median()).values\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.3, random_state=random_state, stratify=y\n",
        "    )\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_sc = scaler.fit_transform(X_train)\n",
        "    X_test_sc = scaler.transform(X_test)\n",
        "\n",
        "    iforest = IsolationForest(**if_params, random_state=random_state)\n",
        "    iforest.fit(X_train_sc)\n",
        "\n",
        "    scores = -iforest.decision_function(X_test_sc)\n",
        "    auc = roc_auc_score(y_test, scores)\n",
        "    return auc\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Cell 7: Load Data and Run Analysis\n",
        "# =============================================================================\n",
        "\n",
        "def run_breast_cancer_analysis():\n",
        "    \"\"\"Main analysis execution.\"\"\"\n",
        "\n",
        "    print(\"=\"*70)\n",
        "    print(\"AD-DIFFI Real Data Analysis: Breast Cancer (Table S2)\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Data setup\n",
        "    data_path = setup_colab_data()\n",
        "\n",
        "    if not data_path:\n",
        "        print(\"[ERROR] Failed to load dataset\")\n",
        "        return\n",
        "\n",
        "    # Load and process data\n",
        "    df_BC = pd.read_csv(data_path)\n",
        "    df_BC_processed = create_binary_features_breast_cancer(df_BC)\n",
        "\n",
        "    # Feature definitions\n",
        "    feature_cols = [c for c in df_BC_processed.columns\n",
        "                    if c not in ['Status', 'Survival Months']]\n",
        "\n",
        "    FEATURE_DEFINITIONS = [\n",
        "        {'name': col, 'type': 'bin' if df_BC_processed[col].nunique() <= 10 else 'cont'}\n",
        "        for col in feature_cols\n",
        "    ]\n",
        "    FEATURE_TYPES = {i: f['type'] for i, f in enumerate(FEATURE_DEFINITIONS)}\n",
        "    FEATURE_NAMES = [f['name'] for f in FEATURE_DEFINITIONS]\n",
        "\n",
        "    print(\"[INFO] Dataset: {} samples, {} features\".format(\n",
        "        len(df_BC_processed), len(feature_cols)\n",
        "    ))\n",
        "    n_cont = sum(1 for f in FEATURE_DEFINITIONS if f['type'] == 'cont')\n",
        "    n_bin = sum(1 for f in FEATURE_DEFINITIONS if f['type'] == 'bin')\n",
        "    print(\"[INFO] Feature types: {} continuous, {} binary\".format(n_cont, n_bin))\n",
        "\n",
        "    # Prepare data\n",
        "    X_data = df_BC_processed[feature_cols].fillna(\n",
        "        df_BC_processed[feature_cols].median()\n",
        "    ).values\n",
        "\n",
        "    # Isolation Forest parameters\n",
        "    IF_PARAMS = {\n",
        "        'n_estimators': 200,\n",
        "        'max_samples': 512,\n",
        "        'contamination': 0.05,\n",
        "        'max_features': 1.0,\n",
        "        'bootstrap': False,\n",
        "    }\n",
        "\n",
        "    # Run comparison\n",
        "    print(\"=\"*70)\n",
        "    print(\"Running feature importance comparison...\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    compare_results = compare_diffi_vs_ad_diffi(\n",
        "        X=X_data,\n",
        "        feature_names=FEATURE_NAMES,\n",
        "        feature_types=FEATURE_TYPES,\n",
        "        if_params=IF_PARAMS,\n",
        "        n_iter_noise=20,\n",
        "        n_iter_main=20,\n",
        "    )\n",
        "\n",
        "    print(\"\\nFeature Importance Comparison (Table S2)\")\n",
        "    print(compare_results.to_markdown(index=False))\n",
        "\n",
        "    # Evaluate IF AUC\n",
        "    print(\"=\"*70)\n",
        "    print(\"Isolation Forest AUC Evaluation\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    df_BC_processed[\"Status_num\"] = (df_BC_processed[\"Status\"] != \"Alive\").astype(int)\n",
        "    y_status = df_BC_processed[\"Status_num\"].values\n",
        "\n",
        "    auc_all = evaluate_if_auc(df_BC_processed, feature_cols, y_status, IF_PARAMS)\n",
        "    print(\"IF AUC (all {} features): {:.4f}\".format(len(feature_cols), auc_all))\n",
        "\n",
        "    top_orig = compare_results.sort_values(\"Original_DIFFI\", ascending=False).head(6)\n",
        "    top_orig_feats = top_orig[\"Feature\"].tolist()\n",
        "    auc_orig6 = evaluate_if_auc(df_BC_processed, top_orig_feats, y_status, IF_PARAMS)\n",
        "    print(\"IF AUC (Original DIFFI top-6): {:.4f}\".format(auc_orig6))\n",
        "\n",
        "    top_ad = compare_results.sort_values(\"AD_DIFFI_RSO_Z\", ascending=False).head(6)\n",
        "    top_ad_feats = top_ad[\"Feature\"].tolist()\n",
        "    auc_ad6 = evaluate_if_auc(df_BC_processed, top_ad_feats, y_status, IF_PARAMS)\n",
        "    print(\"IF AUC (AD-DIFFI top-6): {:.4f}\".format(auc_ad6))\n",
        "\n",
        "    print(\"=\"*70)\n",
        "    print(\"Analysis complete.\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    return compare_results\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Cell 8: Download Results\n",
        "# =============================================================================\n",
        "\n",
        "def save_and_download_results(compare_results: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Save comparison results and prepare for download.\n",
        "\n",
        "    Args:\n",
        "        compare_results: DataFrame with comparison results\n",
        "    \"\"\"\n",
        "    output_filename = 'breast_cancer_analysis_results.csv'\n",
        "    compare_results.to_csv(output_filename, index=False)\n",
        "    print(\"[INFO] Results saved to: {}\".format(output_filename))\n",
        "\n",
        "    # Download in Colab\n",
        "    files.download(output_filename)\n",
        "    print(\"[INFO] Download started...\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# Main Execution (Run in Colab)\n",
        "# =============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run analysis\n",
        "    results = run_breast_cancer_analysis()\n",
        "\n",
        "    # Save and download results\n",
        "    if results is not None:\n",
        "        save_and_download_results(results)"
      ]
    }
  ]
}